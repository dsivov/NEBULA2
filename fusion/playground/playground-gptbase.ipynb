{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4422f0-1ada-4324-8173-2fb89a605b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import subprocess\n",
    "import re\n",
    "import tempfile\n",
    "import itertools\n",
    "import torch\n",
    "import spacy\n",
    "import amrlib\n",
    "import penman\n",
    "import openai\n",
    "\n",
    "from typing import List, Tuple\n",
    "from operator import itemgetter \n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "# import qgrid\n",
    "\n",
    "BASE_DIR = os.path.abspath(os.getcwd()+'/../..')  # /home/gil/dev/NEBULA2/\n",
    "os.chdir(os.getcwd()+'/../..')\n",
    "OPENAI_API_KEY=''\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "from nebula_api.nebula_enrichment_api import *\n",
    "from experts.common.RemoteAPIUtility import RemoteAPIUtility\n",
    "from nebula_api.vlmapi import VLM_API\n",
    "from nebula_api.atomic2020.comet_enrichment_api import *\n",
    "from nebula_api.canonisation_api import CANON_API\n",
    "import nebula_api.playground_api as pg_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd9364-8b09-4c79-9fdc-e9329d40e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nre = NRE_API()\n",
    "api = RemoteAPIUtility()\n",
    "vlm = VLM_API()\n",
    "ascore = CANON_API()\n",
    "# comet = Comet(\"/app/NEBULA2/nebula_api/atomic2020/comet-atomic_2020_BART\")\n",
    "# stog = amrlib.load_stog_model(model_dir=\"/app/NEBULA2/models/model_stog\")\n",
    "# gtos = amrlib.load_gtos_model(model_dir=\"/app/NEBULA2/models/model_gtos\")\n",
    "# model_name = \"Alireza1044/albert-base-v2-cola\" \n",
    "\n",
    "\n",
    "# Download cola model\n",
    "# cola_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243e402-1e54-466f-bf5a-e930ccc6d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst): return [x for l in lst for x in l]\n",
    "\n",
    "def compute_batch_scores(video_emb: torch.Tensor, texts: List[str], normalize=True, **kwargs) -> List[float]:    \n",
    "    emb_batch = vlm.encode_text(texts, **kwargs)\n",
    "    if type(emb_batch) == list:\n",
    "        emb_batch = torch.stack(emb_batch,axis=0)\n",
    "    if normalize:\n",
    "        video_emb = (video_emb / video_emb.norm(2)).squeeze()           # This should be done in vlm\n",
    "        # print(\"normalized video norm: {}\".format(video_emb.norm(2)))\n",
    "        n = (emb_batch * emb_batch).sum(axis=1).sqrt()\n",
    "        emb_batch = emb_batch / n.unsqueeze(1).expand_as(emb_batch)\n",
    "        # print(\"normalized text norms:\")\n",
    "        # for emb in emb_batch:\n",
    "        #     print(emb.norm(2))                        \n",
    "    return (video_emb.unsqueeze(0).expand_as(emb_batch)*emb_batch).sum(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_concat_score(image_emb: torch.Tensor, texts: List[str], join_on=',') -> float:\n",
    "    combined_text = \"\"\n",
    "    for t in [x.strip() for x in texts]:\n",
    "        if t[-1]=='.':\n",
    "            t = t[:-1]       \n",
    "        t+=join_on\n",
    "        t+=' '\n",
    "        combined_text+=t\n",
    "    print(\"Combined: \"+combined_text)\n",
    "    return torch.matmul(image_emb,mdmmt.encode_text(combined_text.strip()) )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ad76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionplace_engine = \"text-curie-001\"\n",
    "actionplace_prompt_template = \"\"\"Combine base, action, and place into one sentence.\n",
    "\n",
    "Base: A man sitting on a bench in front of a building\n",
    "Action: sit and think to himself\n",
    "Place: at a train station in india\n",
    "Combined: A man sits and thinks to himself at a train station in India\n",
    "\n",
    "Base: A woman in a dress is standing in a field\n",
    "Action: make her way to an empty field\n",
    "Place: beside a meadow\n",
    "Combined: A woman in a dress is walking to an empty field, besides a meadow\n",
    "\n",
    "Base: A boat is in the water with a red flag\n",
    "Action: chase the other boat\n",
    "Place: aboard a navy ship\n",
    "Combined: A small navy boat with a red flag speeds and chases another boat\n",
    "\n",
    "Base: A dark image of a creature with a dark background\n",
    "Action: shatter the reflection\n",
    "Place: in a ruined courtyard\n",
    "Combined: A dark image of a creature with a dark background is shattered in a ruined courtyard\n",
    "\n",
    "Base: A woman standing in a doorway in a living room\n",
    "Action: leave for home on her own\n",
    "Place: just inside someone's front door of their house in the living room\n",
    "Combined: A woman standing just inside the front door of someone's house is leaving home on her own\n",
    "\n",
    "Base: A man in a suit and tie standing in front of a wall\n",
    "Action: Take out a passport\n",
    "Place: outside of a terminal baggage claim\n",
    "Combined: A man in a suit and a tie takes out his passport outside of a terminal's baggage claim\n",
    "\n",
    "Base: A woman pushing a suitcase with a man standing behind her\n",
    "Action: set the suitcase by the door\n",
    "Place: in a 19th century house\n",
    "Combined: A woman sets her suitcase by the door in a 19th century house, with a man standing behind her\n",
    "\n",
    "Base: A woman walking down a street in the dark\n",
    "Action: try to evade a car\n",
    "Place: on a narrow street or alley\n",
    "Combined: A woman in a narrow street or alley tries to evade a car\n",
    "\n",
    "Base: A woman walking down a street in the dark\n",
    "Action: trip while she is trying to hurry\n",
    "Place: on a narrow street or alley\n",
    "Combined: A woman trips while she is trying to hurry in a narrow street or alley\n",
    "\n",
    "Base: A car is driving down a dark street at night\n",
    "Action: drive away quickly from the scene\n",
    "Place: outside on a street at night that is filled with police cars\n",
    "Combined:  A car outside on a street filled with police cars is quickly driving away from the scene\n",
    "\n",
    "Base: A woman with a red tie is driving a car\n",
    "Action: hang on tightly to the dashboard\n",
    "Place: in a police car\n",
    "Combined: A woman with a red tie in a police car hangs on tightly to the dashboard\n",
    "\n",
    "Base: A man standing in front of a doorway at night\n",
    "Action: turns on the light\n",
    "Place: just outside of someone's home off of the porch\n",
    "Combined: A man turns on the light just outside of someone's home off of the porch\n",
    "\n",
    "Base: A man standing in front of a doorway at night\n",
    "Action: call out to see if anyone is home\n",
    "Place: just outside of someone's home off of the porch\n",
    "Combined: A man calls out to see if anyone is home, just outside of someone's home off of the porch\n",
    "\n",
    "Base: A young man practices karate\n",
    "Action: leaps over the hedge to the next yard\n",
    "Place: outside someone's home on a porch\n",
    "Combined: A young man  leaps over the hedge to the next yard outside someone's home on a porch\n",
    "\n",
    "Base: Two men standing next to each other in front of a forest\n",
    "Action: stare off into the night sky\n",
    "Place: near a cornfield outside someone's house\n",
    "Combined: Two man stare off into the night sky near a cornfield outside someone's house\n",
    "\n",
    "Base: A man with sunglasses and a yellow jacket standing next to a man with a yellow jacket\n",
    "Action: scan the horizon\n",
    "Place: at a gas station in the desert\n",
    "Combined: At a gas station in the desert, a man with sunglasses and a yellow jacket scans the horizon\n",
    "\n",
    "Base: A man in a suit and tie holding a knife\n",
    "Action: present the notes to his superiors\n",
    "Place: government capitol\n",
    "Combined: A man in a suit and tie presents the notes to his superiors in the government capitol\n",
    "\n",
    "Base: {}\n",
    "Action: {}\n",
    "Place: {}\n",
    "Combined:\"\"\"\n",
    "\n",
    "\n",
    "def gpt_execute(prompt_template, *args, **kwargs):    \n",
    "    prompt = prompt_template.format(*args)\n",
    "    response = openai.Completion.create(prompt=prompt,temperature=0.6, max_tokens=128, **kwargs)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid, elem = ('Movies/114206337', 1)\n",
    "emb_video = vlm.encode_video(mid,elem)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dfe4d185a1b3661f8d189d2dcb52f070789ba26e5d1ea6f8391b638319fa460"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

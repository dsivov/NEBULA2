{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4422f0-1ada-4324-8173-2fb89a605b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import bisect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import subprocess\n",
    "import re\n",
    "import tempfile\n",
    "import itertools\n",
    "import torch\n",
    "import spacy\n",
    "import amrlib\n",
    "import penman\n",
    "import openai\n",
    "\n",
    "from typing import List, Tuple\n",
    "from operator import itemgetter \n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "# import qgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79742a6-1df7-40ae-aec8-c3a3960e81e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.abspath(os.getcwd()+'/../..')  # /home/gil/dev/NEBULA2/\n",
    "os.chdir(os.getcwd()+'/../..')\n",
    "OPENAI_API_KEY=''\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41181f5b-01c9-4ff4-82bf-8819720dc6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nebula_api.nebula_enrichment_api import *\n",
    "from experts.common.RemoteAPIUtility import RemoteAPIUtility\n",
    "from nebula_api.vlmapi import VLM_API\n",
    "from nebula_api.atomic2020.comet_enrichment_api import *\n",
    "from nebula_api.canonisation_api import CANON_API\n",
    "# from nlp_tools.light_house_generator import LightHouseGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9fe9d6-598c-499a-b08f-b548dcc3fdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nebula_api.playground_api as pg_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd9364-8b09-4c79-9fdc-e9329d40e5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nre = NRE_API()\n",
    "api = RemoteAPIUtility()\n",
    "vlm = VLM_API()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dcd694-7362-4aa8-89e8-a6107f71ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = ['Movies/114206952',\n",
    "'Movies/114207205',\n",
    "'Movies/114207398',\n",
    "'Movies/114207499',\n",
    "'Movies/114207361',\n",
    "'Movies/114207740',\n",
    "'Movies/114207908',\n",
    "'Movies/114208744',\n",
    "'Movies/114206724',\n",
    "'Movies/114206548',\n",
    "'Movies/114206264']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Javascript\n",
    "from IPython.display import HTML, display\n",
    "import base64\n",
    "\n",
    "\n",
    "def download_video_file(movie, fname='/tmp/video_file.mp4'):    \n",
    "    if os.path.exists(fname):\n",
    "        os.remove(fname)\n",
    "    query = 'FOR doc IN Movies FILTER doc._id == \"{}\" RETURN doc'.format(movie)\n",
    "    cursor = api.db.aql.execute(query)\n",
    "    url_prefix = \"http://ec2-18-159-140-240.eu-central-1.compute.amazonaws.com:7000/\"\n",
    "    url_link = ''\n",
    "    for doc in cursor:\n",
    "        url_link = url_prefix+doc['url_path']\n",
    "        url_link = url_link.replace(\".avi\", \".mp4\")   \n",
    "        print(url_link)\n",
    "        urllib.request.urlretrieve(url_link, fname) \n",
    "    return fname\n",
    "    # video = cv2.VideoCapture(self.temp_file)\n",
    "    # fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    # return(fps, url_link)\n",
    "\n",
    "\n",
    "\n",
    "def read_video_segm(abspath, t_beg, t_end):\n",
    "    cmd = f'ffmpeg -y -ss {t_beg} -i {abspath} -max_muxing_queue_size 9999  -loglevel error -f mp4 -vf scale=\"(floor(112/ih * iw/2))*2:112\"  -c:a copy  -movflags frag_keyframe+empty_moov -t {t_end - t_beg} pipe:1 -nostats -hide_banner -nostdin'\n",
    "    p = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE)\n",
    "    assert p.returncode == 0, cmd\n",
    "    buf = p.stdout\n",
    "    return buf\n",
    "\n",
    "video_id_cnt = 0    \n",
    "class VideoElem:\n",
    "    def __init__(self, fname, t_start=0, t_end=999):\n",
    "        with open(fname, 'rb') as f:\n",
    "            #data = base64.standard_b64encode(f.read())\n",
    "            buf = read_video_segm(fname, t_start, t_end)\n",
    "            data = base64.standard_b64encode(buf)\n",
    "        global video_id_cnt\n",
    "        video_id_cnt += 1\n",
    "        self.video_id_cnt = video_id_cnt\n",
    "        elem = HTML(f\"\"\"\n",
    "            <video id=\"video_{self.video_id_cnt}\" autoplay loop muted>\n",
    "                <source src=\"data:video/mp4;base64,{data.decode('ascii')}\" type=\"video/mp4\">\n",
    "            </video>        \n",
    "        \"\"\")\n",
    "        display(elem)\n",
    "    \n",
    "    def hide(self):\n",
    "        js = f'$(\"#video_{self.video_id_cnt}\").hide()'\n",
    "        display(Javascript(js))\n",
    "        \n",
    "    def show(self):\n",
    "        js = f'$(\"#video_{self.video_id_cnt}\").show()'\n",
    "        display(Javascript(js))\n",
    "\n",
    "    def remove(self):\n",
    "        js = f'$(\"#video_{self.video_id_cnt}\").remove()'\n",
    "        display(Javascript(js))\n",
    "        \n",
    "def mdmmt_video_encode(start_f, stop_f, path='/tmp/video_file.mp4', freq=24):\n",
    "        t_start = start_f//freq\n",
    "        t_end = stop_f//freq\n",
    "        if t_start == t_end:\n",
    "            t_start = t_start - 1\n",
    "        print(\"Start/stop\", t_start, \" \", t_end)\n",
    "        if (t_end - t_start) >= 1:\n",
    "            vemb = mdmmt.encode_video(\n",
    "                mdmmt.vggish_model,  # adio modality\n",
    "                mdmmt.vmz_model,  # video modality\n",
    "                mdmmt.clip_model,  # image modality\n",
    "                mdmmt.model_vid,  # aggregator\n",
    "                path, t_start, t_end)\n",
    "            return(vemb)\n",
    "        else:\n",
    "            print(\"Stage too short\")\n",
    "            return(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb78a56f-a2e5-40f5-8e85-2d94d3f357a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_to_concepts(frame)-> List:\n",
    "    def transform_concept(c):\n",
    "        exp = re.compile(r\"^([a-zA-z]+)(\\d*)$\")\n",
    "        r = exp.match(c)\n",
    "        return r.group(1) if r else c\n",
    "        \n",
    "    pre_concepts = set(frame['tracker_description']).union(set(frame['step_description'])).union(set(frame['simulated_expert']))\n",
    "    concepts = list(set(map(transform_concept,pre_concepts)))\n",
    "    return concepts\n",
    "\n",
    "def kgbart_fusion(frames) -> (List[str], List[str]):\n",
    "    h, outname = tempfile.mkstemp(text=True)\n",
    "    os.close(h)\n",
    "    h, fname = tempfile.mkstemp(text=True)\n",
    "    os.close(h)\n",
    "    KGBART_MAIN = BASE_DIR+'/kgbart/KGBART/KGBART_training/decode_seq2seq.py'\n",
    "    KGBART_CC_DIR = BASE_DIR+'/kgbart/downloaded/commongen_dataset'\n",
    "    KGBART_MODEL_DIR = BASE_DIR+'/kgbart/output/best_model/model.best.bin'\n",
    "    options = {\n",
    "        'data_dir': KGBART_CC_DIR,\n",
    "        'output_dir': os.path.dirname(outname),\n",
    "        'input_file': fname,\n",
    "        'model_recover_path': KGBART_MODEL_DIR,\n",
    "        'output_file': os.path.basename(outname),\n",
    "        'split': 'dev',\n",
    "        'beam_size': 5,\n",
    "        'forbid_duplicate_ngrams': True\n",
    "    }\n",
    "    all_concepts = []\n",
    "    with open(fname, 'w') as f:\n",
    "        for frame in frames:\n",
    "            concepts = frame_to_concepts(frame)\n",
    "            all_concepts.append(', '.join(concepts))\n",
    "            f.write(' '.join(concepts)+'\\n')\n",
    "        \n",
    "    # write expert tokens to input file\n",
    "    \n",
    "    cmdline = 'python '+KGBART_MAIN+' '+ ' '.join(['--{} {}'.format(k,v) for (k,v) in options.items()]) + '>/dev/null 2>&1'\n",
    "    os.system(cmdline)\n",
    "    with open(outname,'r') as f:\n",
    "        rc = f.readlines()\n",
    "    os.unlink(outname)\n",
    "    os.unlink(fname)\n",
    "    return all_concepts, rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243e402-1e54-466f-bf5a-e930ccc6d00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lst): return [x for l in lst for x in l]\n",
    "\n",
    "def compute_batch_scores(video_emb: torch.Tensor, texts: List[str], normalize=True, **kwargs) -> List[float]:    \n",
    "    emb_batch = vlm.encode_text(texts, **kwargs)\n",
    "    if type(emb_batch) == list:\n",
    "        emb_batch = torch.stack(emb_batch,axis=0)\n",
    "    if normalize:\n",
    "        video_emb = video_emb / video_emb.norm(2)\n",
    "        # print(\"normalized video norm: {}\".format(video_emb.norm(2)))\n",
    "        n = (emb_batch * emb_batch).sum(axis=1).sqrt()\n",
    "        emb_batch = emb_batch / n.unsqueeze(1).expand_as(emb_batch)\n",
    "        # print(\"normalized text norms:\")\n",
    "        # for emb in emb_batch:\n",
    "        #     print(emb.norm(2))                        \n",
    "    return (video_emb.unsqueeze(0).expand_as(emb_batch)*emb_batch).sum(dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "def compute_concat_score(image_emb: torch.Tensor, texts: List[str], join_on=',') -> float:\n",
    "    combined_text = \"\"\n",
    "    for t in [x.strip() for x in texts]:\n",
    "        if t[-1]=='.':\n",
    "            t = t[:-1]       \n",
    "        t+=join_on\n",
    "        t+=' '\n",
    "        combined_text+=t\n",
    "    print(\"Combined: \"+combined_text)\n",
    "    return torch.matmul(image_emb,mdmmt.encode_text(combined_text.strip()) )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_concept(c):\n",
    "    exp = re.compile(r\"^([a-zA-z]+)-?(\\d*)$\")\n",
    "    r = exp.match(c)\n",
    "    return r.group(1) if r else c\n",
    "\n",
    "class ConceptManager:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def ground_concept(concept):\n",
    "        return transform_concept(concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5004e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityManager:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "    def similarity(self, c1, c2):\n",
    "        if type(c2) is not list:\n",
    "            c2 = [c2]   \n",
    "        a = self.nlp(c1)\n",
    "        targets = self.nlp(' '.join(c2))\n",
    "        return [a.similarity(x) for x in targets]\n",
    "\n",
    "\n",
    "smanager = SimilarityManager()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d497a",
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = lambda x: np.exp(x)/sum(np.exp(x))\n",
    "\n",
    "class SubsetOptimization:\n",
    "    def __init__(self, video_emb, experts: List, candidates_strings: List[str]):\n",
    "        self.stog = amrlib.load_stog_model(model_dir=\"/app/NEBULA2/models/model_stog\")\n",
    "        self.video_emb = video_emb\n",
    "        self.initial_temp = 10\n",
    "        self.final_temp = .05\n",
    "        self.alpha = 0.01\n",
    "        self.theta = 0.5\n",
    "        self.experts = experts\n",
    "        self.candidates_strings = candidates_strings\n",
    "        self.candidates_amr_strings = self.stog.parse_sents(self.candidates_strings) \n",
    "        self.candidates = self.candidates_amr_strings\n",
    "        self.candidates_amrs = [penman.decode(x) for x in self.candidates_amr_strings]\n",
    "        self.candidates_similarity = compute_batch_scores(self.video_emb, self.candidates_strings)             \n",
    "        self.opt_results = []\n",
    "        self.smanager = SimilarityManager()\n",
    "\n",
    "        self.coverage_matrix = np.zeros([len(self.experts),len(self.candidates)])\n",
    "        self.coverage_matrix[:] = np.nan\n",
    "        for i in range(len(experts)):\n",
    "            for j in range(len(candidates_strings)):\n",
    "                self.coverage_matrix[i][j]=self.concept_amr_similarity(self.experts[i],self.candidates_amrs[j])\n",
    "        self.max_size = int(len(self.experts)*1.5)\n",
    "\n",
    "    def concept_amr_similarity(self, concept, amr):\n",
    "        insts = [ConceptManager.ground_concept(x.target) for x in amr.instances()]\n",
    "        sims = self.smanager.similarity(concept, insts)\n",
    "        return max(sims)\n",
    "\n",
    "    def get_coverage(self,i,j):        \n",
    "        if np.isnan(self.coverage_matrix[i][j]):\n",
    "            self.coverage_matrix[i][j] = self.concept_amr_similarity(self.experts[i],self.candidates_amrs[j])\n",
    "        return self.coverage_matrix[i][j]\n",
    "\n",
    "    def get_expert_coverage(self,state):\n",
    "        return self.coverage_matrix[:,state].max(axis=1)\n",
    "\n",
    "    def get_state_coverage(self,state) -> float:\n",
    "        print(\"State coverage for {}:\".format(state))\n",
    "        print(self.get_expert_coverage(state))\n",
    "        return np.mean(self.get_expert_coverage(state))\n",
    "\n",
    "    # def get_state_coverage(self, state: List[int]) -> float:\n",
    "    #     experts_coverage = [max([self.get_coverage(i,j) for j in state]) for i in range(len(self.experts))]    # A list of partial coverege        \n",
    "    #     return sum(experts_coverage) / len(self.experts)\n",
    "\n",
    "    def get_cost(self, state: List[int]) -> float:\n",
    "        if not state:\n",
    "            return 0\n",
    "        coverage_score = self.get_state_coverage(state)           \n",
    "        similarity_score = self.candidates_similarity[state].mean().item()\n",
    "        return -(coverage_score + self.theta*similarity_score)\n",
    "\n",
    "    # state here is assumed (and guaranteed on return) to be -sorted-\n",
    "    def get_candidate(self, state: List[int]) -> List[int]:\n",
    "        def compute_state_arrays(s):\n",
    "            print(\"Computing arrays for state: \")\n",
    "            print(s)\n",
    "            s_score = self.candidates_similarity[s]\n",
    "            s_coverage = self.coverage_matrix.mean(axis=0)[s]\n",
    "            s_max_coverage = self.coverage_matrix.max(axis=0)[s]\n",
    "            s_fitscore = s_coverage+self.theta*s_score\n",
    "\n",
    "            return (s_score,s_coverage,s_max_coverage,s_fitscore)\n",
    "\n",
    "        if not state:\n",
    "            print(\"Empty state\")\n",
    "            return [random.randint(0,len(self.candidates_strings)-1)]\n",
    "            \n",
    "        rc = state.copy()\n",
    "        s = np.array(state)\n",
    "        s_score, s_coverage, s_max_coverage, s_fitscore = compute_state_arrays(s)\n",
    "               \n",
    "        if len(state) == self.max_size:\n",
    "            print(\"Maximum state size, removing\")\n",
    "            idx = np.argmin(s_fitscore)\n",
    "            del rc[idx]\n",
    "            return rc\n",
    "            \n",
    "        remove_sentence = random.random()<self.get_state_coverage(state)        \n",
    "        print(\"coverage of {} is {}, remove?{}\".format(state,self.get_state_coverage(state),remove_sentence))\n",
    "        if remove_sentence:             # We decide to remove a sentence from the set\n",
    "            print(\"Removing\")\n",
    "            probs = softmax(-s_fitscore)\n",
    "            idx = np.random.multinomial(1,probs).argmax()\n",
    "            del rc[idx]                   \n",
    "        else:                           # Add a sentence from the outside\n",
    "            print(\"Adding\")\n",
    "            anti_state = []\n",
    "            for i in range(len(self.candidates_strings)):\n",
    "                if not i in state:\n",
    "                    anti_state.append(i)\n",
    "            s1 = np.array(anti_state)\n",
    "            s1_score, s1_coverage, s1_max_coverage, s1_fitscore = compute_state_arrays(s1)\n",
    "            # Pick an expert to try and cover\n",
    "            probs = softmax(self.get_expert_coverage(s)*10)         # Coverage is in (0,1), so we use low temprature\n",
    "            expert_to_cover = np.random.multinomial(1,probs).argmax()\n",
    "            probs = softmax(self.coverage_matrix[expert_to_cover][s1]*10)\n",
    "            idx_to_add = np.random.multinomial(1,probs).argmax()\n",
    "            bisect.insort(rc,anti_state[idx_to_add])\n",
    "            \n",
    "        return rc\n",
    "\n",
    "\n",
    "\n",
    "    def get_scored_permutations(self, k):\n",
    "        n = len(self.candidates)\n",
    "        return [(x,self.get_cost(list(x))) for x in itertools.permutations(range(n),k)]\n",
    "        \n",
    "    def simulated_annealing(self, initial_state):\n",
    "        self.opt_results = []\n",
    "        current_temp = self.initial_temp\n",
    "\n",
    "       # Start by initializing the current state with the initial state\n",
    "        current_state = initial_state\n",
    "\n",
    "        while current_temp > self.final_temp:\n",
    "            next_cand = self.get_candidate(current_state)\n",
    "\n",
    "            print(\"current cost: {} ({}). Candidate cost: {} ({})\".format(self.get_cost(current_state),current_state,self.get_cost(next_cand),next_cand))\n",
    "\n",
    "            # Check if next_cand is best so far\n",
    "            cost_diff = self.get_cost(current_state) - self.get_cost(next_cand)\n",
    "\n",
    "            # if the new solution is better, accept it\n",
    "            if cost_diff > 0:\n",
    "                current_state = next_cand\n",
    "            # if the new solution is not better, accept it with a probability of e^(-cost/temp)\n",
    "            else:\n",
    "                print(\"chance to move: {}\".format(math.exp(cost_diff / current_temp)))\n",
    "                if random.uniform(0, 1) < math.exp(cost_diff / current_temp):\n",
    "                    current_state = next_cand\n",
    "            # decrement the temperature\n",
    "            current_temp -= self.alpha\n",
    "            self.opt_results.append(-self.get_cost(current_state))\n",
    "\n",
    "        return current_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bd7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Get a list of 1-item dictionaries, return a list of the values\n",
    "'''\n",
    "\n",
    "def rearrange_concepts(concepts):\n",
    "    return concepts[:7]\n",
    "\n",
    "def permute_sentence(pen, concepts):    \n",
    "    def replace_instance(g: penman.Graph, changes: List[tuple[int,str]]) -> penman.Graph :\n",
    "        amr_copy = penman.Graph(triples=g.triples, epidata=g.epidata)\n",
    "        for (i,val) in changes:\n",
    "            b = list(amr_copy.triples[i])\n",
    "            b[2] = val\n",
    "            amr_copy.triples[i] = tuple(b)\n",
    "        return amr_copy\n",
    "\n",
    "    concepts = {k: rearrange_concepts(v) for (k,v) in concepts.items()}    \n",
    "    insts_list = []\n",
    "    rc = []\n",
    "    concept_classes = []\n",
    "    dims = []\n",
    "    for i,triplet in enumerate(pen.triples):\n",
    "        if triplet[1] == ':instance':\n",
    "            entity_class = ascore.get_class_of_entity(transform_concept(triplet[2]))\n",
    "            concept_class = concepts[entity_class] if entity_class in concepts.keys() else []\n",
    "            if triplet[2] not in concept_class:\n",
    "                concept_class.append(triplet[2])\n",
    "            insts_list.append((i,triplet, entity_class))\n",
    "            dims.append(range(len(concept_class)))\n",
    "            concept_classes.append(concept_class)\n",
    "    prods = itertools.product(*dims)\n",
    "    for cand in prods:        \n",
    "        changes = [(insts_list[i][0],concept_classes[i][d]) for (i,d) in enumerate(cand)]\n",
    "        rc.append(replace_instance(pen,changes))\n",
    "    \n",
    "    return rc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7005bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_variables(pen, prefix='n_'):\n",
    "    new_triples = []\n",
    "    for t in pen.triples:\n",
    "        triple = list(t)\n",
    "        if triple[0] in pen.variables():\n",
    "            triple[0] = prefix+triple[0]\n",
    "        if triple[2] in pen.variables():\n",
    "            triple[2] = prefix+triple[2]\n",
    "        new_triples.append(tuple(triple))\n",
    "    amr_copy = penman.Graph(triples=new_triples)\n",
    "    return amr_copy            \n",
    "    \n",
    "\n",
    "def combine_place(base_amr, place_amr):\n",
    "    place_copy = rename_variables(place_amr)\n",
    "    combined_triples = base_amr.triples + place_copy.triples \n",
    "    combined_triples.append(tuple([base_amr.top, ':location', place_copy.top]))\n",
    "    return penman.Graph(triples=combined_triples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f23445",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_combine(base: str, action: str, place: str, engine=\"text-davinci-001\"):\n",
    "    prompt_template = \"\"\"Combine base, action, and place into one sentence.\n",
    "\n",
    "Base: A man sitting on a bench in front of a building\n",
    "Action: sit and think to himself\n",
    "Place: at a train station in india\n",
    "Combined: A man sits and thinks to himself at a train station in India\n",
    "\n",
    "Base: A woman in a dress is standing in a field\n",
    "Action: make her way to an empty field\n",
    "Place: beside a meadow\n",
    "Combined: A woman in a dress is walking to an empty field, besides a meadow\n",
    "\n",
    "Base: A boat is in the water with a red flag\n",
    "Action: chase the other boat\n",
    "Place: aboard a navy ship\n",
    "Combined: A small navy boat with a red flag speeds and chases another boat\n",
    "\n",
    "Base: A dark image of a creature with a dark background\n",
    "Action: shatter the reflection\n",
    "Place: in a ruined courtyard\n",
    "Combined: A dark image of a creature with a dark background is shattered in a ruined courtyard\n",
    "\n",
    "Base: A woman standing in a doorway in a living room\n",
    "Action: leave for home on her own\n",
    "Place: just inside someone's front door of their house in the living room\n",
    "Combined: A woman standing just inside the front door of someone's house is leaving home on her own\n",
    "\n",
    "Base: A man in a suit and tie standing in front of a wall\n",
    "Action: Take out a passport\n",
    "Place: outside of a terminal baggage claim\n",
    "Combined: A man in a suit and a tie takes out his passport outside of a terminal's baggage claim\n",
    "\n",
    "Base: A woman pushing a suitcase with a man standing behind her\n",
    "Action: set the suitcase by the door\n",
    "Place: in a 19th century house\n",
    "Combined: A woman sets her suitcase by the door in a 19th century house, with a man standing behind her\n",
    "\n",
    "Base: A woman walking down a street in the dark\n",
    "Action: try to evade a car\n",
    "Place: on a narrow street or alley\n",
    "Combined: A woman in a narrow street or alley tries to evade a car\n",
    "\n",
    "Base: A woman walking down a street in the dark\n",
    "Action: trip while she is trying to hurry\n",
    "Place: on a narrow street or alley\n",
    "Combined: A woman trips while she is trying to hurry in a narrow street or alley\n",
    "\n",
    "Base: A car is driving down a dark street at night\n",
    "Action: drive away quickly from the scene\n",
    "Place: outside on a street at night that is filled with police cars\n",
    "Combined:  A car outside on a street filled with police cars is quickly driving away from the scene\n",
    "\n",
    "Base: A woman with a red tie is driving a car\n",
    "Action: hang on tightly to the dashboard\n",
    "Place: in a police car\n",
    "Combined: A woman with a red tie in a police car hangs on tightly to the dashboard\n",
    "\n",
    "Base: A man standing in front of a doorway at night\n",
    "Action: turns on the light\n",
    "Place: just outside of someone's home off of the porch\n",
    "Combined: A man turns on the light just outside of someone's home off of the porch\n",
    "\n",
    "Base: A man standing in front of a doorway at night\n",
    "Action: call out to see if anyone is home\n",
    "Place: just outside of someone's home off of the porch\n",
    "Combined: A man calls out to see if anyone is home, just outside of someone's home off of the porch\n",
    "\n",
    "Base: A young man practices karate\n",
    "Action: leaps over the hedge to the next yard\n",
    "Place: outside someone's home on a porch\n",
    "Combined: A young man  leaps over the hedge to the next yard outside someone's home on a porch\n",
    "\n",
    "Base: Two men standing next to each other in front of a forest\n",
    "Action: stare off into the night sky\n",
    "Place: near a cornfield outside someone's house\n",
    "Combined: Two man stare off into the night sky near a cornfield outside someone's house\n",
    "\n",
    "Base: A man with sunglasses and a yellow jacket standing next to a man with a yellow jacket\n",
    "Action: scan the horizon\n",
    "Place: at a gas station in the desert\n",
    "Combined: At a gas station in the desert, a man with sunglasses and a yellow jacket scans the horizon\n",
    "\n",
    "Base: A man in a suit and tie holding a knife\n",
    "Action: present the notes to his superiors\n",
    "Place: government capitol\n",
    "Combined: A man in a suit and tie presents the notes to his superiors in the government capitol\n",
    "\n",
    "Base: {}\n",
    "Action: {}\n",
    "Place: {}\n",
    "Combined:\"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(base, action, place)\n",
    "    response = openai.Completion.create(engine=engine,prompt=prompt,temperature=0.6, max_tokens=128)\n",
    "    return response\n",
    "\n",
    "def compute_gpt_lighthouses(mid, scene_element, max_lighthouses=1000, **kwargs):\n",
    "    results = []\n",
    "    data = nre.get_groundings_from_db(mid,scene_element)\n",
    "    bases = data['base']\n",
    "    places = data['places'][:5]\n",
    "    actions = data['actions'][:5]\n",
    "    for base in bases:\n",
    "        for place in places:\n",
    "            for action in actions:\n",
    "                rc = gpt_combine(base,action,place, **kwargs)\n",
    "                results.append((rc.choices[0].text,(base,action,place)))\n",
    "                if len(results) >= max_lighthouses:\n",
    "                    return results\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6337b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid = 'Movies/114206337' # 'Movies/114208338'\n",
    "# mid = 'Movies/114208338' # 'Movies/114208338'\n",
    "# mid = 'Movies/114207550' # 'Movies/114208338'\n",
    "elem = 0\n",
    "movie_info = api.get_movie_info(mid)\n",
    "emb_image = vlm.encode_video(mid,elem,class_name='mdmmt_mean')\n",
    "data = nre.get_groundings_from_db(mid,elem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf0833-1f5a-4cdc-afbd-6a207e4ba374",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = data['base']\n",
    "places = data['places']\n",
    "# experts = data['experts']\n",
    "actions = data['actions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d09a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# engines are ada/babbage/curie/davinci\n",
    "rc_babbage = compute_gpt_lighthouses(mid,elem,engine=\"text-babbage-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cab304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_ada = compute_gpt_lighthouses(mid,elem,engine=\"text-ada-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_babbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba25fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x[0] for x in rc_ada]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dfe4d185a1b3661f8d189d2dcb52f070789ba26e5d1ea6f8391b638319fa460"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
